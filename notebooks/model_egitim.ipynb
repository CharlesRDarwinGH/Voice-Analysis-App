{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ses Kayıtlarını 1,5 Saniyelik Chunklara Ayırma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tüm dosyalar başarıyla 1.5 saniyelik parçalara bölündü.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# WAV dosyalarının bulunduğu dizin\n",
    "wav_dir = \"ses/dosyalarının/yolu\"\n",
    "chunk_dir = \"C:/.../chunks\"\n",
    "os.makedirs(chunk_dir, exist_ok=True)  # Chunk'lar için çıkış dizini oluştur\n",
    "\n",
    "# WAV dosyalarının listesi\n",
    "wav_files = os.listdir(wav_dir)\n",
    "\n",
    "# Her WAV dosyasını işleyin\n",
    "for wav_file in wav_files:\n",
    "    if wav_file.endswith('.wav'):\n",
    "        # WAV dosyasını yükle\n",
    "        audio = AudioSegment.from_wav(os.path.join(wav_dir, wav_file))\n",
    "        # 1.5 saniyelik parçalara böl (milisaniye cinsinden 1500ms)\n",
    "        chunk_length_ms = 1500  # 1.5 saniye\n",
    "        chunks = [audio[i:i + chunk_length_ms] for i in range(0, len(audio), chunk_length_ms)]\n",
    "        \n",
    "        # Parçaları kaydet\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_name = f\"{os.path.splitext(wav_file)[0]}_chunk_{i}.wav\"\n",
    "            chunk.export(os.path.join(chunk_dir, chunk_name), format=\"wav\")\n",
    "\n",
    "print(\"Tüm dosyalar başarıyla 1.5 saniyelik parçalara bölündü.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seslerin MFCC Feature'larını Çıkarma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "# Ses dosyalarının bulunduğu dizin\n",
    "chunk_dir = \"C:/.../chunks\"\n",
    "\n",
    "# MFCC özelliklerinin kaydedileceği dizin\n",
    "mfcc_dir = 'C:/.../mfcc_chunks'\n",
    "os.makedirs(mfcc_dir, exist_ok=True)\n",
    "\n",
    "# MFCC parametreleri\n",
    "n_mfcc = 128\n",
    "frame_length = 25  # milisaniye\n",
    "frame_stride = 10  # milisaniye\n",
    "max_length = 128\n",
    "\n",
    "# Dosyaları işleme\n",
    "for chunk_file in os.listdir(chunk_dir):\n",
    "    if chunk_file.endswith('.wav'):\n",
    "        chunk_path = os.path.join(chunk_dir, chunk_file)\n",
    "        print(f\"İşleniyor: {chunk_file}\")\n",
    "        try:\n",
    "            # Ses dosyasını yükleme\n",
    "            y, sr = librosa.load(chunk_path, sr=None)\n",
    "            hop_length = int(frame_stride * sr / 1000)\n",
    "            n_fft = int(frame_length * sr / 1000)\n",
    "\n",
    "            # MFCC çıkarma\n",
    "            mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc, hop_length=hop_length, n_fft=n_fft)\n",
    "\n",
    "            # Sabit uzunlukta MFCC oluşturma\n",
    "            if mfcc.shape[1] > max_length:\n",
    "                mfcc = mfcc[:, :max_length]\n",
    "            elif mfcc.shape[1] < max_length:\n",
    "                pad_width = max_length - mfcc.shape[1]\n",
    "                mfcc = np.pad(mfcc, ((0, 0), (0, pad_width)), mode='constant')\n",
    "\n",
    "            # MFCC'yi düzleştirme\n",
    "            mfcc_flat = mfcc.flatten()\n",
    "\n",
    "            # MFCC dosyasını kaydet\n",
    "            mfcc_file = os.path.join(mfcc_dir, chunk_file.replace('.wav', '.npy'))\n",
    "            np.save(mfcc_file, mfcc_flat)\n",
    "        except Exception as e:\n",
    "            print(f\"{chunk_file} işlenirken hata oluştu: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Eğitimi ve Model Doğruluğunu Görme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from collections import Counter\n",
    "import joblib\n",
    "\n",
    "# MFCC dosyalarının bulunduğu dizin\n",
    "# MFCC dosyalarının bulunduğu dizin\n",
    "mfcc_dir = 'C:/.../mfcc_chunks'\n",
    "\n",
    "# Veriyi yükleme\n",
    "X, y = [], []\n",
    "for mfcc_file in os.listdir(mfcc_dir):\n",
    "    if mfcc_file.endswith('.npy'):\n",
    "        try:\n",
    "            mfcc_path = os.path.join(mfcc_dir, mfcc_file)\n",
    "            mfcc = np.load(mfcc_path)\n",
    "            X.append(mfcc.flatten())  # Düzleştirme işlemi burada yapılıyor\n",
    "            label = mfcc_file.split('_')[0]\n",
    "            y.append(label)\n",
    "        except Exception as e:\n",
    "            print(f\"{mfcc_file} yüklenirken hata oluştu: {e}\")\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Etiketleri sayısal değerlere dönüştürme\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Sınıf ağırlıklarını hesaplama\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "sample_weights = np.array([class_weights[class_label] for class_label in y])\n",
    "\n",
    "# Sınıf dağılımını kontrol et\n",
    "print(\"Sınıf dağılımı:\", Counter(y))\n",
    "\n",
    "# Normalizasyon\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Veri kümesini eğitim ve test kümelerine ayırma\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "\n",
    "# MLP Modeli\n",
    "model = MLPClassifier(hidden_layer_sizes=(128, 64), activation='relu', solver='adam', max_iter=1000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Modelin doğruluğunu değerlendirme\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print(f\"Model Doğruluğu: {accuracy:.2f}\")\n",
    "\n",
    "# Sınıflandırma Raporu\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "# Karışıklık Matrisi\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Karışıklık Matrisi:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Çapraz doğrulama\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "print(f\"Çapraz Doğrulama Sonuçları: {scores}\")\n",
    "print(f\"Ortalama Doğruluk: {scores.mean():.2f}\")\n",
    "\n",
    "# Modeli kaydetme\n",
    "joblib.dump(model, 'C:/.../mlp_model.pkl')\n",
    "joblib.dump(label_encoder, 'C:/.../label_encoder.pkl')\n",
    "joblib.dump(scaler, 'C:/.../scaler.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Doğruluğunu Görme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, label_encoder):\n",
    "    # Test veri seti üzerinde doğruluk değerlendirmesi\n",
    "    accuracy = model.score(X_test, y_test)\n",
    "    print(f\"Test Doğruluğu (Accuracy): {accuracy:.4f}\")\n",
    "    \n",
    "    # Sınıflandırma Raporu ve Karışıklık Matrisi\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"Sınıflandırma Raporu:\")\n",
    "    if label_encoder:\n",
    "        target_names = label_encoder.classes_ if hasattr(label_encoder, 'classes_') else None\n",
    "        print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "    else:\n",
    "        print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    print(\"Karışıklık Matrisi:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Örnek kullanım\n",
    "evaluate_model(model, X_test, y_test, label_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tahmin Yaptırma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "import joblib\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import time\n",
    "\n",
    "# Eğitilen model, scaler ve label encoder yolları\n",
    "model_kayit_yolu = 'C:/...mlp_model.pkl'\n",
    "label_encoder_yolu = 'C:/.../label_encoder.pkl'\n",
    "scaler_yolu = 'C:/.../scaler.pkl'\n",
    "\n",
    "# Model, LabelEncoder ve Scaler'ı yükle\n",
    "model = joblib.load(model_kayit_yolu)\n",
    "label_encoder = joblib.load(label_encoder_yolu)\n",
    "scaler = joblib.load(scaler_yolu)\n",
    "\n",
    "# Sınıf isimlerini LabelEncoder'dan al\n",
    "sinif_isimleri = label_encoder.classes_\n",
    "print(\"Sınıflar:\", sinif_isimleri)\n",
    "\n",
    "# Mikrofon parametreleri\n",
    "saniye_basina_ornek = 44100\n",
    "saniye = 5\n",
    "kanal_sayisi = 1\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        print(\"Konuşun...\")\n",
    "        # Ses kaydı\n",
    "        ses = sd.rec(int(saniye_basina_ornek * saniye), samplerate=saniye_basina_ornek, channels=kanal_sayisi, dtype='float32')\n",
    "        sd.wait()\n",
    "        \n",
    "        # Dinamik dosya ismi\n",
    "        kayit_yolu = f\"kayit_{int(time.time())}.wav\"\n",
    "        sf.write(kayit_yolu, np.squeeze(ses), saniye_basina_ornek)\n",
    "        \n",
    "        # MFCC çıkarma\n",
    "        y, sr = librosa.load(kayit_yolu, sr=saniye_basina_ornek)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=128, hop_length=512, n_fft=2048)\n",
    "        mfcc = np.mean(mfcc.T, axis=1)\n",
    "\n",
    "        # Normalizasyon (Scaler ile)\n",
    "        mfcc_normalized = scaler.transform(mfcc.reshape(1, -1))\n",
    "\n",
    "        # Model tahmini\n",
    "        tahmin_indeksi = model.predict(mfcc_normalized)[0]\n",
    "        tahmin_isim = sinif_isimleri[tahmin_indeksi]\n",
    "\n",
    "        print(\"Tahmin edilen kişi:\", tahmin_isim)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nProgram durduruldu. Çıkış yapılıyor...\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
